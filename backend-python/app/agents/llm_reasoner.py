"""
LLM æ¨ç†å™¨ - Agent çš„å¤§è„‘

è´Ÿè´£ Agent çš„æ€è€ƒå’Œå†³ç­–:
1. åˆ†æç”¨æˆ·é—®é¢˜
2. å†³å®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
3. é€‰æ‹©åˆé€‚çš„å·¥å…·
4. ç”Ÿæˆæ¨ç†é“¾
"""

import json
import logging
import httpx
from typing import Dict, Any, List, Optional
from langchain_core.language_models.base import BaseLanguageModel
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

from ..services.ias_proxy import IASProxyService
from ..models.ias import IASChatRequest, IASChatMessage, ChatRole
from ..core.database import get_db
from ..models.llm_model import LLMModel

logger = logging.getLogger(__name__)


class LLMReasoner:
    """
    LLM æ¨ç†å™¨

    ä½¿ç”¨ LangChain BaseLanguageModel æ¥å£å°è£… LLM è°ƒç”¨,
    æ”¯æŒå¤šç§ LLM æä¾›å•† (OpenAI, Anthropic, Custom)
    """

    def __init__(self, llm_model_id: Optional[str] = None):
        """
        åˆå§‹åŒ– LLM æ¨ç†å™¨

        Args:
            llm_model_id: LLM æ¨¡å‹ ID (å¦‚æœä¸º None,ä½¿ç”¨é»˜è®¤æ¨¡å‹)
        """
        self.llm_model_id = llm_model_id
        self.ias_proxy = IASProxyService()
        self.llm: Optional[BaseLanguageModel] = None
        self._model_config: Optional[Dict[str, Any]] = None

        logger.info(f"ğŸ§  LLMReasoner åˆå§‹åŒ– (model_id={llm_model_id})")

    async def initialize(self, db) -> bool:
        """
        ä»æ•°æ®åº“åŠ è½½ LLM æ¨¡å‹é…ç½®

        Args:
            db: æ•°æ®åº“ä¼šè¯

        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        try:
            # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ ID,ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
            if self.llm_model_id:
                model = db.query(LLMModel).filter(
                    LLMModel.id == self.llm_model_id,
                    LLMModel.is_active == True
                ).first()
            else:
                # å¦åˆ™ä½¿ç”¨é»˜è®¤çš„æ´»è·ƒæ¨¡å‹
                model = db.query(LLMModel).filter(
                    LLMModel.is_active == True
                ).order_by(LLMModel.created_at).first()

            if not model:
                logger.error(f"âŒ æœªæ‰¾åˆ°å¯ç”¨çš„ LLM æ¨¡å‹ (model_id={self.llm_model_id})")
                return False

            self._model_config = {
                "id": model.id,
                "name": model.name,
                "display_name": model.display_name,
                "model_type": model.model_type,
                "api_url": model.api_url,
                "api_key": model.api_key,
                "temperature": model.temperature / 100 if model.temperature else 0.7,
            }

            logger.info(f"âœ… LLMReasoner ä½¿ç”¨æ¨¡å‹: {model.display_name} ({model.name})")
            return True

        except Exception as e:
            logger.error(f"âŒ LLMReasoner åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def reason(
        self,
        question: str,
        available_tools: List[str],
        conversation_history: List[Dict[str, Any]],
        previous_steps: List[Dict[str, Any]],
        agent_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨ç†å†³ç­–

        Args:
            question: ç”¨æˆ·é—®é¢˜
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            conversation_history: å¯¹è¯å†å²
            previous_steps: ä¹‹å‰çš„æ‰§è¡Œæ­¥éª¤
            agent_config: Agent é…ç½®

        Returns:
            å†³ç­–ç»“æœ:
            {
                "reasoning": "æ¨ç†é“¾",
                "tool": "å·¥å…·åç§°" æˆ– None,
                "parameters": {å‚æ•°},
                "confidence": 0.95,
                "should_continue": True
            }
        """
        # âœ… Phase 1: æ£€æµ‹ç”¨æˆ·æ˜¯å¦é€‰æ‹©äº†çŸ¥è¯†åº“
        logger.info(f"ğŸ” [DEBUG] agent_config ç±»å‹: {type(agent_config)}")
        logger.info(f"ğŸ” [DEBUG] agent_config å†…å®¹: {agent_config}")

        knowledge_base_ids = agent_config.get("knowledge_base_ids", [])
        has_knowledge_base = len(knowledge_base_ids) > 0

        logger.info(f"ğŸ” [DEBUG] knowledge_base_ids: {knowledge_base_ids}")
        logger.info(f"ğŸ” [DEBUG] has_knowledge_base: {has_knowledge_base}")

        if has_knowledge_base:
            logger.info(f"ğŸ“š [æ¨ç†] æ£€æµ‹åˆ°ç”¨æˆ·é€‰æ‹©äº†çŸ¥è¯†åº“: {knowledge_base_ids}")
        else:
            logger.info(f"ğŸ“š [æ¨ç†] ç”¨æˆ·æœªé€‰æ‹©çŸ¥è¯†åº“")

        # 1. æ„å»ºæ¨ç†æç¤ºè¯
        prompt = self._build_reasoning_prompt(
            question=question,
            available_tools=available_tools,
            conversation_history=conversation_history,
            previous_steps=previous_steps,
            agent_config=agent_config,
            has_knowledge_base=has_knowledge_base  # â† ä¼ é€’çŸ¥è¯†åº“çŠ¶æ€
        )

        # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºæç¤ºè¯å†…å®¹
        logger.info(f"ğŸ“ [æç¤ºè¯] é•¿åº¦: {len(prompt)} å­—ç¬¦")
        if "âš ï¸ é‡è¦æç¤ºï¼šç”¨æˆ·å·²ç»é€‰æ‹©äº†çŸ¥è¯†åº“" in prompt:
            logger.info("âœ… [æç¤ºè¯] åŒ…å«çŸ¥è¯†åº“æç¤º")
        else:
            logger.warning("âš ï¸  [æç¤ºè¯] ä¸åŒ…å«çŸ¥è¯†åº“æç¤ºï¼")
        logger.info(f"ğŸ“„ [æç¤ºè¯] å†…å®¹é¢„è§ˆ:\n{prompt[:500]}...")

        # 2. è°ƒç”¨ LLM è¿›è¡Œæ¨ç†
        try:
            response = await self._call_llm(
                prompt=prompt,
                question=question,
                agent_config=agent_config,
                has_knowledge_base=has_knowledge_base  # â† æ–°å¢å‚æ•°
            )

            # 3. è§£æ LLM å“åº”
            decision = self._parse_decision(response, available_tools)
            return decision

        except Exception as e:
            logger.error(f"âŒ LLM æ¨ç†å¤±è´¥: {e}")
            # é™çº§: è¿”å›é»˜è®¤å†³ç­–
            return self._fallback_decision(question, available_tools, previous_steps)

    def _build_reasoning_prompt(
        self,
        question: str,
        available_tools: List[str],
        conversation_history: List[Dict[str, Any]],
        previous_steps: List[Dict[str, Any]],
        agent_config: Dict[str, Any],
        has_knowledge_base: bool = False
    ) -> str:
        """æ„å»ºæ¨ç†æç¤ºè¯"""

        # å·¥å…·æè¿°
        tool_descriptions = self._get_tool_descriptions(available_tools)

        # å¯¹è¯å†å²æ‘˜è¦
        history_summary = ""
        if conversation_history:
            recent = conversation_history[-3:]  # æœ€è¿‘ 3 è½®
            # å¤„ç† LangChain Message å¯¹è±¡æˆ–å­—å…¸
            formatted_messages = []
            for msg in recent:
                # å¦‚æœæ˜¯ LangChain Message å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                if hasattr(msg, 'content') and hasattr(msg, 'type'):
                    role = msg.type  # 'human', 'ai', 'system'
                    content = msg.content
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                else:
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                formatted_messages.append(f"{role}: {str(content)[:100]}")
            history_summary = "\n".join(formatted_messages)

        # ä¹‹å‰æ­¥éª¤æ‘˜è¦
        steps_summary = ""
        if previous_steps:
            steps_summary = "\n".join([
                f"- æ­¥éª¤ {i+1}: ä½¿ç”¨ {step.get('tool', 'unknown')} å·¥å…·"
                for i, step in enumerate(previous_steps[-3:])
            ])

        # âœ… çŸ¥è¯†åº“çŠ¶æ€æç¤º
        knowledge_base_hint = ""
        if has_knowledge_base:
            knowledge_base_hint = """
âš ï¸ é‡è¦æç¤ºï¼šç”¨æˆ·å·²ç»é€‰æ‹©äº†çŸ¥è¯†åº“ï¼å¦‚æœé—®é¢˜æ¶‰åŠä¿¡æ¯æ£€ç´¢ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ knowledge_search å·¥å…·ã€‚
"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹,éœ€è¦åˆ†æç”¨æˆ·é—®é¢˜å¹¶å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·ã€‚

# å¯ç”¨å·¥å…·
{tool_descriptions}

# ç”¨æˆ·é—®é¢˜
{question}

# å¯¹è¯å†å²
{history_summary if history_summary else "(æ— )"}

# ä¹‹å‰çš„æ­¥éª¤
{steps_summary if steps_summary else "(æ— )"}

{knowledge_base_hint}

# ä»»åŠ¡
è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”:

## æ¨ç†
[ä½ çš„æ¨ç†è¿‡ç¨‹ - åˆ†æé—®é¢˜éœ€è¦ä»€ä¹ˆä¿¡æ¯,æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·]

## å·¥å…·å†³ç­–
[å¦‚æœéœ€è¦å·¥å…·,å¡«å†™å·¥å…·åç§°;å¦‚æœä¸éœ€è¦,å¡«å†™ "none"]

## å·¥å…·å‚æ•°
[å¦‚æœé€‰æ‹©äº†å·¥å…·,ä»¥ JSON æ ¼å¼æä¾›å‚æ•°;å¦åˆ™å¡«å†™ "{{}}"]

## ç½®ä¿¡åº¦
[ä½ å¯¹è¿™ä¸ªå†³ç­–çš„ä¿¡å¿ƒç¨‹åº¦,0-1 ä¹‹é—´çš„æµ®ç‚¹æ•°]

## ç»§ç»­æ‰§è¡Œ
[å·¥å…·æ‰§è¡Œåæ˜¯å¦éœ€è¦ç»§ç»­æ€è€ƒ true/false]

æ³¨æ„:
- å¦‚æœç”¨æˆ·å·²ç»é€‰æ‹©äº†çŸ¥è¯†åº“ï¼Œä¼˜å…ˆä½¿ç”¨ knowledge_search å·¥å…·
- å¦‚æœé—®é¢˜å¯ä»¥ç›´æ¥å›ç­”,ä¸è¦ä½¿ç”¨å·¥å…·
- å¦‚æœé—®é¢˜æ¶‰åŠè®¡ç®—ã€æœç´¢ã€æ—¶é—´ç­‰,ä½¿ç”¨ç›¸åº”å·¥å…·
- ç½®ä¿¡åº¦åº”è¯¥åŸºäºé—®é¢˜æ˜¯å¦æ¸…æ™°ã€å·¥å…·æ˜¯å¦åŒ¹é…
- å¦‚æœä½¿ç”¨å·¥å…·åå¯èƒ½éœ€è¦æ›´å¤šä¿¡æ¯,è®¾ç½®ç»§ç»­æ‰§è¡Œä¸º true
"""

        return prompt

    def _get_tool_descriptions(self, tools: List[str]) -> str:
        """è·å–å·¥å…·æè¿°"""
        descriptions = {
            "calculator": "è®¡ç®—å™¨ - æ‰§è¡Œæ•°å­¦è®¡ç®—,ä¾‹å¦‚: '2+2', '100*5.5'",
            "datetime": "æ—¥æœŸæ—¶é—´ - è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´",
            "knowledge_search": "çŸ¥è¯†åº“æœç´¢ - åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯,éœ€è¦ query å‚æ•°",
            "code_executor": "ä»£ç æ‰§è¡Œ - å®‰å…¨æ‰§è¡Œ Python ä»£ç å¹¶è¿”å›ç»“æœ",
            "browser": "æµè§ˆå™¨ - è®¿é—®ç½‘é¡µã€æœç´¢ä¿¡æ¯ã€æŠ“å–å†…å®¹",
            "file": "æ–‡ä»¶æ“ä½œ - è¯»å–ã€å†™å…¥ã€åˆ—å‡ºæ–‡ä»¶",
        }

        lines = []
        for tool in tools:
            desc = descriptions.get(tool, "æœªçŸ¥å·¥å…·")
            lines.append(f"- {tool}: {desc}")

        return "\n".join(lines) if lines else "æ— å¯ç”¨å·¥å…·"

    async def _call_llm(
        self,
        prompt: str,
        question: str,
        agent_config: Dict[str, Any],
        has_knowledge_base: bool = False  # â† æ–°å¢å‚æ•°
    ) -> str:
        """
        è°ƒç”¨ LLM APIï¼ˆçœŸæ­£çš„ LLM æ¨ç†ï¼‰

        Args:
            prompt: å®Œæ•´çš„æç¤ºè¯ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
            question: åŸå§‹ç”¨æˆ·é—®é¢˜ï¼ˆç”¨äºè§„åˆ™åˆ¤æ–­ï¼‰
            agent_config: Agent é…ç½®
            has_knowledge_base: ç”¨æˆ·æ˜¯å¦é€‰æ‹©äº†çŸ¥è¯†åº“
        """
        try:
            # æ£€æŸ¥æ¨¡å‹é…ç½®
            if not self._model_config:
                logger.error("âŒ LLM æ¨¡å‹æœªé…ç½®ï¼Œæ— æ³•è°ƒç”¨ API")
                # é™çº§åˆ°è§„åˆ™å¼•æ“
                return await self._call_ias_direct(
                    request=None,
                    question=question,
                    has_knowledge_base=has_knowledge_base
                )

            # è·å– API Key å’Œ URL
            api_key = self._model_config.get("api_key")
            api_url = self._model_config.get("api_url")
            model_name = self._model_config.get("name")

            if not api_key or not api_url:
                logger.error(f"âŒ LLM API é…ç½®ä¸å®Œæ•´: api_key={bool(api_key)}, api_url={bool(api_url)}")
                # é™çº§åˆ°è§„åˆ™å¼•æ“
                return await self._call_ias_direct(
                    request=None,
                    question=question,
                    has_knowledge_base=has_knowledge_base
                )

            logger.info(f"ğŸ¤– [LLM è°ƒç”¨] ä½¿ç”¨æ¨¡å‹: {model_name}")
            logger.info(f"ğŸ“ [LLM è°ƒç”¨] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"ğŸŒ [LLM è°ƒç”¨] API URL: {api_url}")

            # æ„å»ºè¯·æ±‚ä½“ï¼ˆæ™ºè°± AI OpenAI å…¼å®¹æ ¼å¼ï¼‰
            request_data = {
                "model": model_name,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹,æ“…é•¿æ¨ç†å’Œå·¥å…·ä½¿ç”¨å†³ç­–ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„æ ¼å¼å›ç­”ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "stream": False
            }

            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }

            # ç›´æ¥è°ƒç”¨ API
            async with httpx.AsyncClient(timeout=30.0) as client:
                logger.info("ğŸŒ [LLM è°ƒç”¨] å‘é€è¯·æ±‚...")
                response = await client.post(
                    api_url,
                    json=request_data,
                    headers=headers
                )

                if response.status_code != 200:
                    logger.error(f"âŒ [LLM è°ƒç”¨] API è¿”å›é”™è¯¯: {response.status_code}")
                    logger.error(f"âŒ [LLM è°ƒç”¨] å“åº”å†…å®¹: {response.text[:500]}")
                    # é™çº§åˆ°è§„åˆ™å¼•æ“
                    return await self._call_ias_direct(
                        request=None,
                        question=question,
                        has_knowledge_base=has_knowledge_base
                    )

                # è§£æå“åº”
                response_json = response.json()
                logger.info(f"âœ… [LLM è°ƒç”¨] API è°ƒç”¨æˆåŠŸ")

                # æ™ºè°± AI å“åº”æ ¼å¼: {"choices": [{"message": {"content": "..."}}]}
                if "choices" in response_json and len(response_json["choices"]) > 0:
                    content = response_json["choices"][0]["message"]["content"]
                    logger.info(f"âœ… [LLM å“åº”] å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    logger.info(f"ğŸ“„ [LLM å“åº”] å†…å®¹é¢„è§ˆ: {content[:200]}...")
                    return content
                else:
                    logger.error(f"âŒ [LLM å“åº”] æ ¼å¼é”™è¯¯: {response_json}")
                    # é™çº§åˆ°è§„åˆ™å¼•æ“
                    return await self._call_ias_direct(
                        request=None,
                        question=question,
                        has_knowledge_base=has_knowledge_base
                    )

        except Exception as e:
            logger.error(f"âŒ LLM API è°ƒç”¨å¤±è´¥: {e}")
            logger.error(f"âŒ [é”™è¯¯è¯¦æƒ…] {type(e).__name__}: {str(e)}")
            # é™çº§åˆ°è§„åˆ™å¼•æ“
            logger.info("â¬‡ï¸  [é™çº§] ä½¿ç”¨è§„åˆ™å¼•æ“ä½œä¸ºé™çº§æ–¹æ¡ˆ")
            return await self._call_ias_direct(
                request=None,
                question=question,
                has_knowledge_base=has_knowledge_base
            )

    async def _call_ias_direct(
        self,
        request: Optional[IASChatRequest],
        question: str,
        has_knowledge_base: bool = False  # â† æ–°å¢å‚æ•°
    ) -> str:
        """
        é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨è§„åˆ™å¼•æ“ï¼ˆå½“ LLM API ä¸å¯ç”¨æ—¶ï¼‰

        Args:
            request: IAS è¯·æ±‚å¯¹è±¡ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
            question: åŸå§‹ç”¨æˆ·é—®é¢˜ï¼ˆç”¨äºè§„åˆ™åˆ¤æ–­ï¼‰
            has_knowledge_base: ç”¨æˆ·æ˜¯å¦é€‰æ‹©äº†çŸ¥è¯†åº“

        æ³¨æ„ï¼šè¿™æ˜¯é™çº§æ–¹æ¡ˆï¼Œä¼˜å…ˆä½¿ç”¨çœŸæ­£çš„ LLM API
        """
        # è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„ IAS API
        # æš‚æ—¶ä½¿ç”¨è§„åˆ™å¼•æ“è¿›è¡Œé™çº§å¤„ç†

        logger.info(f"ğŸ“‹ [è§„åˆ™å¼•æ“] åˆ†æé—®é¢˜: {question[:50]}...")
        logger.info(f"ğŸ“š [è§„åˆ™å¼•æ“] çŸ¥è¯†åº“çŠ¶æ€: {'å·²é€‰æ‹©' if has_knowledge_base else 'æœªé€‰æ‹©'}")

        # âœ… Phase 1 å…³é”®ä¿®å¤: å¦‚æœç”¨æˆ·é€‰æ‹©äº†çŸ¥è¯†åº“ï¼Œä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“æœç´¢
        if has_knowledge_base:
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] æ£€æµ‹åˆ°çŸ¥è¯†åº“é€‰æ‹©ï¼Œä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“æœç´¢")
            # ä½¿ç”¨å®é™…çš„æŸ¥è¯¢å†…å®¹ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
            query_json = json.dumps({"query": question}, ensure_ascii=False)
            return f"""## æ¨ç†
ç”¨æˆ·é€‰æ‹©äº†çŸ¥è¯†åº“ï¼Œåº”è¯¥ä½¿ç”¨çŸ¥è¯†åº“æœç´¢å·¥å…·æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯

## å·¥å…·å†³ç­–
knowledge_search

## å·¥å…·å‚æ•°
{query_json}

## ç½®ä¿¡åº¦
0.95

## ç»§ç»­æ‰§è¡Œ
false
"""

        # ç®€å•è§„åˆ™åˆ¤æ–­ (åŸºäºåŸå§‹é—®é¢˜)
        question_lower = question.lower()

        # è§„åˆ™ 1: è®¡ç®—ç±»é—®é¢˜
        if any(keyword in question for keyword in ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/", "="]):
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] åŒ¹é…è§„åˆ™: è®¡ç®—")
            expression_json = json.dumps({"expression": question}, ensure_ascii=False)
            return f"""## æ¨ç†
ç”¨æˆ·é—®é¢˜æ¶‰åŠæ•°å­¦è®¡ç®—,éœ€è¦ä½¿ç”¨è®¡ç®—å™¨å·¥å…·

## å·¥å…·å†³ç­–
calculator

## å·¥å…·å‚æ•°
{expression_json}

## ç½®ä¿¡åº¦
0.95

## ç»§ç»­æ‰§è¡Œ
false
"""

        # è§„åˆ™ 2: æ—¶é—´æ—¥æœŸç±»é—®é¢˜
        elif any(keyword in question for keyword in ["æ—¶é—´", "æ—¥æœŸ", "å‡ ç‚¹", "ç°åœ¨", "ä»Šå¤©", "å‡ ç‚¹äº†"]):
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] åŒ¹é…è§„åˆ™: æ—¶é—´æ—¥æœŸ")
            return """## æ¨ç†
ç”¨æˆ·è¯¢é—®å½“å‰æ—¶é—´æˆ–æ—¥æœŸ,éœ€è¦ä½¿ç”¨æ—¥æœŸæ—¶é—´å·¥å…·

## å·¥å…·å†³ç­–
datetime

## å·¥å…·å‚æ•°
{}

## ç½®ä¿¡åº¦
0.95

## ç»§ç»­æ‰§è¡Œ
false
"""

        # è§„åˆ™ 3: æœç´¢ç±»é—®é¢˜
        elif any(keyword in question_lower for keyword in ["æœç´¢", "search", "æŸ¥æ‰¾", "çŸ¥è¯†", "ä¿¡æ¯", "ä»€ä¹ˆæ˜¯"]):
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] åŒ¹é…è§„åˆ™: æœç´¢")
            query_json = json.dumps({"query": question}, ensure_ascii=False)
            return f"""## æ¨ç†
ç”¨æˆ·éœ€è¦æŸ¥æ‰¾ä¿¡æ¯,åº”è¯¥ä½¿ç”¨çŸ¥è¯†åº“æœç´¢å·¥å…·

## å·¥å…·å†³ç­–
knowledge_search

## å·¥å…·å‚æ•°
{query_json}

## ç½®ä¿¡åº¦
0.90

## ç»§ç»­æ‰§è¡Œ
false
"""

        # è§„åˆ™ 4: é—®å€™ç±»é—®é¢˜ï¼ˆç›´æ¥å›ç­”ï¼‰
        elif any(keyword in question for keyword in ["ä½ å¥½", "hello", "hi", "å—¨", "æ‚¨å¥½"]):
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] åŒ¹é…è§„åˆ™: é—®å€™")
            return """## æ¨ç†
ç”¨æˆ·åœ¨æ‰“æ‹›å‘¼,åº”è¯¥å‹å¥½å›åº”,ä¸éœ€è¦ä½¿ç”¨å·¥å…·

## å·¥å…·å†³ç­–
none

## å·¥å…·å‚æ•°
{}

## ç½®ä¿¡åº¦
0.95

## ç»§ç»­æ‰§è¡Œ
false
"""

        else:
            # é»˜è®¤: ä¸ä½¿ç”¨å·¥å…·,å°è¯•ç›´æ¥å›ç­”
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] ä½¿ç”¨é»˜è®¤è§„åˆ™")
            return """## æ¨ç†
é—®é¢˜å¯ä»¥ç›´æ¥å›ç­”æˆ–ä¸éœ€è¦ç‰¹å®šå·¥å…·,å°†å°è¯•æä¾›æœ‰å¸®åŠ©çš„å›å¤

## å·¥å…·å†³ç­–
none

## å·¥å…·å‚æ•°
{}

## ç½®ä¿¡åº¦
0.70

## ç»§ç»­æ‰§è¡Œ
false
"""

    def _parse_decision(self, response: str, available_tools: List[str]) -> Dict[str, Any]:
        """è§£æ LLM å“åº”ä¸ºå†³ç­–"""
        try:
            # æå–å„ä¸ªéƒ¨åˆ†
            reasoning = self._extract_section(response, "æ¨ç†")
            tool = self._extract_section(response, "å·¥å…·å†³ç­–").strip().lower()
            parameters_str = self._extract_section(response, "å·¥å…·å‚æ•°")
            confidence = float(self._extract_section(response, "ç½®ä¿¡åº¦").strip())
            should_continue_str = self._extract_section(response, "ç»§ç»­æ‰§è¡Œ").strip().lower()
            should_continue = should_continue_str in ["true", "yes", "æ˜¯"]

            # è§£æå‚æ•° JSON
            try:
                parameters = json.loads(parameters_str) if parameters_str != "{}" else {}
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ å·¥å…·å‚æ•° JSON è§£æå¤±è´¥: {parameters_str}")
                parameters = {}

            # éªŒè¯å·¥å…·æ˜¯å¦å¯ç”¨
            if tool and tool != "none" and tool not in available_tools:
                logger.warning(f"âš ï¸ LLM é€‰æ‹©çš„å·¥å…· {tool} ä¸å¯ç”¨,é™çº§åˆ° none")
                tool = "none"

            return {
                "reasoning": reasoning,
                "tool": tool if tool != "none" else None,
                "parameters": parameters,
                "confidence": confidence,
                "should_continue": should_continue
            }

        except Exception as e:
            logger.error(f"âŒ è§£æå†³ç­–å¤±è´¥: {e}")
            # è¿”å›å®‰å…¨é»˜è®¤å€¼
            return {
                "reasoning": response[:200] if response else "è§£æå¤±è´¥",
                "tool": None,
                "parameters": {},
                "confidence": 0.5,
                "should_continue": False
            }

    def _extract_section(self, text: str, section_name: str) -> str:
        """æå–æ–‡æœ¬ä¸­çš„æŸä¸ªéƒ¨åˆ†"""
        # æŸ¥æ‰¾ ## section_name
        marker = f"## {section_name}"
        if marker not in text:
            return ""

        start = text.index(marker) + len(marker)
        # æŸ¥æ‰¾ä¸‹ä¸€ä¸ª ## æˆ–æ–‡æœ¬ç»“æŸ
        next_marker = text.find("##", start)
        if next_marker == -1:
            return text[start:].strip()
        else:
            return text[start:next_marker].strip()

    def _fallback_decision(
        self,
        question: str,
        available_tools: List[str],
        previous_steps: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        é™çº§å†³ç­– (å½“ LLM è°ƒç”¨å¤±è´¥æ—¶)

        ä½¿ç”¨ç®€å•è§„åˆ™è¿›è¡Œå·¥å…·é€‰æ‹©
        """
        question_lower = question.lower()

        # è§„åˆ™ 1: è®¡ç®—
        if any(word in question for word in ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/"]) and "calculator" in available_tools:
            return {
                "reasoning": "é—®é¢˜æ¶‰åŠæ•°å­¦è®¡ç®—,ä½¿ç”¨è®¡ç®—å™¨",
                "tool": "calculator",
                "parameters": {"expression": question},
                "confidence": 0.70,
                "should_continue": False
            }

        # è§„åˆ™ 2: æ—¥æœŸæ—¶é—´
        if any(word in question for word in ["æ—¶é—´", "æ—¥æœŸ", "å‡ ç‚¹", "ä»Šå¤©", "ç°åœ¨"]) and "datetime" in available_tools:
            return {
                "reasoning": "é—®é¢˜è¯¢é—®å½“å‰æ—¶é—´,ä½¿ç”¨æ—¥æœŸæ—¶é—´å·¥å…·",
                "tool": "datetime",
                "parameters": {},
                "confidence": 0.80,
                "should_continue": False
            }

        # è§„åˆ™ 3: çŸ¥è¯†åº“æœç´¢
        if any(word in question for word in ["æœç´¢", "æŸ¥æ‰¾", "çŸ¥è¯†", "ä¿¡æ¯"]) and "knowledge_search" in available_tools:
            return {
                "reasoning": "é—®é¢˜éœ€è¦æœç´¢ä¿¡æ¯,ä½¿ç”¨çŸ¥è¯†åº“",
                "tool": "knowledge_search",
                "parameters": {"query": question},
                "confidence": 0.70,
                "should_continue": False
            }

        # é»˜è®¤: ä¸ä½¿ç”¨å·¥å…·
        return {
            "reasoning": "é—®é¢˜å¯ä»¥ç›´æ¥å›ç­”,ä¸éœ€è¦å·¥å…·",
            "tool": None,
            "parameters": {},
            "confidence": 0.60,
            "should_continue": False
        }

    async def generate_final_answer(
        self,
        question: str,
        tools_called: List[Dict[str, Any]],
        agent_config: Dict[str, Any]
    ) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

        Args:
            question: ç”¨æˆ·é—®é¢˜
            tools_called: å·¥å…·è°ƒç”¨è®°å½•
            agent_config: Agent é…ç½®

        Returns:
            æœ€ç»ˆç­”æ¡ˆ
        """
        # è¿™ä¸ªæ–¹æ³•åœ¨ agent_service.py ä¸­å·²å®ç°
        # è¿™é‡Œåªæ˜¯æ¥å£å®šä¹‰
        raise NotImplementedError("ä½¿ç”¨ AgentService._generate_final_answer ä»£æ›¿")
