"""
LLM æ¨ç†å™¨ - Agent çš„å¤§è„‘

è´Ÿè´£ Agent çš„æ€è€ƒå’Œå†³ç­–:
1. åˆ†æç”¨æˆ·é—®é¢˜
2. å†³å®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
3. é€‰æ‹©åˆé€‚çš„å·¥å…·
4. ç”Ÿæˆæ¨ç†é“¾
"""

import json
import logging
import httpx
import re
from typing import Dict, Any, List, Optional
from langchain_core.language_models.base import BaseLanguageModel
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

from ..services.ias_proxy import IASProxyService
from ..models.ias import IASChatRequest, IASChatMessage, ChatRole
from ..core.database import get_db
from ..models.llm_model import LLMModel

logger = logging.getLogger(__name__)


class LLMReasoner:
    """
    LLM æ¨ç†å™¨

    ä½¿ç”¨ LangChain BaseLanguageModel æ¥å£å°è£… LLM è°ƒç”¨,
    æ”¯æŒå¤šç§ LLM æä¾›å•† (OpenAI, Anthropic, Custom)
    """

    def __init__(self, llm_model_id: Optional[str] = None):
        """
        åˆå§‹åŒ– LLM æ¨ç†å™¨

        Args:
            llm_model_id: LLM æ¨¡å‹ ID (å¦‚æœä¸º None,ä½¿ç”¨é»˜è®¤æ¨¡å‹)
        """
        self.llm_model_id = llm_model_id
        self.ias_proxy = IASProxyService()
        self.llm: Optional[BaseLanguageModel] = None
        self._model_config: Optional[Dict[str, Any]] = None

        logger.info(f"ğŸ§  LLMReasoner åˆå§‹åŒ– (model_id={llm_model_id})")

    async def initialize(self, db) -> bool:
        """
        ä»æ•°æ®åº“åŠ è½½ LLM æ¨¡å‹é…ç½®

        Args:
            db: æ•°æ®åº“ä¼šè¯

        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        try:
            # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ ID,ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
            if self.llm_model_id:
                model = db.query(LLMModel).filter(
                    LLMModel.id == self.llm_model_id,
                    LLMModel.is_active == True
                ).first()
            else:
                # å¦åˆ™ä½¿ç”¨é»˜è®¤çš„æ´»è·ƒæ¨¡å‹
                model = db.query(LLMModel).filter(
                    LLMModel.is_active == True
                ).order_by(LLMModel.created_at).first()

            if not model:
                logger.error(f"âŒ æœªæ‰¾åˆ°å¯ç”¨çš„ LLM æ¨¡å‹ (model_id={self.llm_model_id})")
                return False

            self._model_config = {
                "id": model.id,
                "name": model.name,
                "display_name": model.display_name,
                "model_type": model.model_type,
                "api_url": model.api_url,
                "api_key": model.api_key,
                "temperature": model.temperature / 100 if model.temperature else 0.7,
            }

            logger.info(f"âœ… LLMReasoner ä½¿ç”¨æ¨¡å‹: {model.display_name} ({model.name})")
            return True

        except Exception as e:
            logger.error(f"âŒ LLMReasoner åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def reason(
        self,
        question: str,
        available_tools: List[str],
        conversation_history: List[Dict[str, Any]],
        previous_steps: List[Dict[str, Any]],
        agent_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨ç†å†³ç­–ï¼ˆçº¯ LLM é©±åŠ¨ï¼Œæ— è§„åˆ™å¼•æ“ï¼‰

        Args:
            question: ç”¨æˆ·é—®é¢˜
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            conversation_history: å¯¹è¯å†å²
            previous_steps: ä¹‹å‰çš„æ‰§è¡Œæ­¥éª¤
            agent_config: Agent é…ç½®

        Returns:
            å†³ç­–ç»“æœ
        """
        # æ£€æµ‹çŸ¥è¯†åº“çŠ¶æ€
        knowledge_base_ids = agent_config.get("knowledge_base_ids", [])
        has_knowledge_base = len(knowledge_base_ids) > 0

        logger.info(f"ğŸ¤– [LLM æ¨ç†] agent_config: {agent_config}")
        logger.info(f"ğŸ¤– [LLM æ¨ç†] knowledge_base_ids: {knowledge_base_ids}")
        logger.info(f"ğŸ¤– [LLM æ¨ç†] has_knowledge_base: {has_knowledge_base}")
        logger.info(f"ğŸ¤– [LLM æ¨ç†] ä½¿ç”¨çº¯ LLM åˆ†æï¼ˆæ— è§„åˆ™å¼•æ“ï¼‰")

        try:
            # æ„å»ºå¢å¼ºçš„ prompt
            prompt = self._build_reasoning_prompt(
                question, available_tools, conversation_history,
                previous_steps, agent_config, has_knowledge_base
            )

            # è°ƒç”¨ LLM
            response = await self._call_llm(prompt, question, agent_config, has_knowledge_base)

            # è§£æå†³ç­–
            decision = self._parse_decision(response, available_tools)

            logger.info(f"âœ… [LLM æ¨ç†] å·¥å…·: {decision.get('tool')}, ç½®ä¿¡åº¦: {decision.get('confidence', 0)}")

            return decision

        except Exception as e:
            logger.error(f"âŒ LLM æ¨ç†å¤±è´¥: {e}")
            # é™çº§åˆ°ç®€å•è§„åˆ™ï¼ˆä»…ä½œä¸ºåå¤‡ï¼‰
            return self._fallback_decision(question, available_tools, previous_steps)

    def _build_reasoning_prompt(
        self,
        question: str,
        available_tools: List[str],
        conversation_history: List[Dict[str, Any]],
        previous_steps: List[Dict[str, Any]],
        agent_config: Dict[str, Any],
        has_knowledge_base: bool = False
    ) -> str:
        """æ„å»ºæ¨ç†æç¤ºè¯"""

        # å·¥å…·æè¿°
        tool_descriptions = self._get_tool_descriptions(available_tools)

        # å¯¹è¯å†å²æ‘˜è¦
        history_summary = ""
        if conversation_history:
            recent = conversation_history[-3:]  # æœ€è¿‘ 3 è½®
            # å¤„ç† LangChain Message å¯¹è±¡æˆ–å­—å…¸
            formatted_messages = []
            for msg in recent:
                # å¦‚æœæ˜¯ LangChain Message å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                if hasattr(msg, 'content') and hasattr(msg, 'type'):
                    role = msg.type  # 'human', 'ai', 'system'
                    content = msg.content
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                else:
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                formatted_messages.append(f"{role}: {str(content)[:100]}")
            history_summary = "\n".join(formatted_messages)

        # ä¹‹å‰æ­¥éª¤æ‘˜è¦
        steps_summary = ""
        if previous_steps:
            steps_summary = "\n".join([
                f"- æ­¥éª¤ {i+1}: ä½¿ç”¨ {step.get('tool', 'unknown')} å·¥å…·"
                for i, step in enumerate(previous_steps[-3:])
            ])

        # âœ… çŸ¥è¯†åº“çŠ¶æ€æç¤º
        knowledge_base_hint = ""
        if has_knowledge_base:
            knowledge_base_hint = """
âœ… ç”¨æˆ·å·²é€‰æ‹©çŸ¥è¯†åº“ï¼šå¯ä»¥ä½¿ç”¨ knowledge_search å·¥å…·æ£€ç´¢ä¿¡æ¯
"""
        else:
            knowledge_base_hint = """
âŒ ç”¨æˆ·æœªé€‰æ‹©çŸ¥è¯†åº“ï¼šç¦æ­¢ä½¿ç”¨ knowledge_search å·¥å…·ï¼ˆè¯·ç›´æ¥å›ç­”æˆ–ä½¿ç”¨å…¶ä»–å·¥å…·ï¼‰
"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½ä½“å·¥å…·å†³ç­–ä¸“å®¶ã€‚

# å¯ç”¨å·¥å…·
{tool_descriptions}

# ç”¨æˆ·é—®é¢˜
{question}

{knowledge_base_hint}

# ğŸ¯ æ ¸å¿ƒå†³ç­–åŸåˆ™

1. **ç†è§£ç”¨æˆ·æ„å›¾**: ä»”ç»†åˆ†æç”¨æˆ·çœŸæ­£æƒ³è¦åšä»€ä¹ˆ
2. **å·¥å…·ä¼˜å…ˆçº§**:
   - ç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨æŸä¸ªå·¥å…· â†’ ä¼˜å…ˆä½¿ç”¨è¯¥å·¥å…·
   - è®¡ç®—é—®é¢˜ â†’ ä¼˜å…ˆä½¿ç”¨ code_executorï¼ˆæ›´ç²¾ç¡®ï¼‰
   - æ—¶é—´/æ—¥æœŸ â†’ datetime
   - ç½‘é¡µæ“ä½œ â†’ browser
     - ç›´æ¥è®¿é—®URL â†’ action: "scrape"
     - æœç´¢å…³é”®è¯ â†’ action: "search"
   - ä¿¡æ¯æ£€ç´¢ â†’ ä»…åœ¨ç”¨æˆ·å·²é€‰æ‹©çŸ¥è¯†åº“æ—¶ä½¿ç”¨ knowledge_search
3. **ç›´æ¥å›ç­”åœºæ™¯**:
   - é—®å€™ã€é—²èŠä¸éœ€è¦å·¥å…·
   - ç”¨æˆ·æœªé€‰æ‹©çŸ¥è¯†åº“æ—¶ï¼Œä¿¡æ¯ç±»é—®é¢˜åº”ç›´æ¥å›ç­”
4. **å‚æ•°å®Œæ•´æ€§**: ç¡®ä¿å·¥å…·æ‰€éœ€å‚æ•°å®Œæ•´ä¸”æ­£ç¡®

# âš ï¸ é‡è¦ï¼šå¿…é¡»è¿”å›å®Œæ•´çš„ JSON æ ¼å¼

{{
  "reasoning": "è¯¦ç»†åˆ†æï¼šä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªå·¥å…·ï¼Œç”¨æˆ·çš„æ„å›¾æ˜¯ä»€ä¹ˆ",
  "tool": "å·¥å…·åç§°æˆ– null",
  "parameters": {{"å‚æ•°å": "å‚æ•°å€¼"}},
  "confidence": 0.95,
  "should_continue": false
}}

# ğŸ’¡ å®Œæ•´ç¤ºä¾‹ï¼ˆå¿…é¡»åŒ…å«æ‰€æœ‰å­—æ®µï¼‰

1. è®¡ç®—é—®é¢˜ï¼š
{{
  "reasoning": "ç”¨æˆ·è¦æ±‚è¿›è¡Œæ•°å­¦è®¡ç®—ï¼Œä½¿ç”¨ code_executor å·¥å…·æ‰§è¡Œ Python ä»£ç ",
  "tool": "code_executor",
  "parameters": {{"code": "print(100 * 200)"}},
  "confidence": 0.95,
  "should_continue": false
}}

2. æŸ¥è¯¢æ—¶é—´ï¼š
{{
  "reasoning": "ç”¨æˆ·è¯¢é—®å½“å‰æ—¶é—´ï¼Œä½¿ç”¨ datetime å·¥å…·è·å–",
  "tool": "datetime",
  "parameters": {{}},
  "confidence": 0.95,
  "should_continue": false
}}

3. é—®å€™ï¼š
{{
  "reasoning": "ç”¨æˆ·åœ¨æ‰“æ‹›å‘¼ï¼Œç›´æ¥å‹å¥½å›åº”ï¼Œä¸éœ€è¦ä½¿ç”¨å·¥å…·",
  "tool": null,
  "parameters": {{}},
  "confidence": 0.95,
  "should_continue": false
}}

4. ç›´æ¥è®¿é—®ç½‘é¡µï¼š
{{
  "reasoning": "ç”¨æˆ·è¦æ±‚æ€»ç»“æŒ‡å®šç½‘é¡µå†…å®¹ï¼Œä½¿ç”¨ browser å·¥å…·çš„ scrape æ“ä½œç›´æ¥è·å–ç½‘é¡µ",
  "tool": "browser",
  "parameters": {{"action": "scrape", "url": "https://example.com"}},
  "confidence": 0.95,
  "should_continue": false
}}

5. æœç´¢ç½‘é¡µï¼š
{{
  "reasoning": "ç”¨æˆ·è¦æ±‚æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œä½¿ç”¨ browser å·¥å…·çš„ search æ“ä½œ",
  "tool": "browser",
  "parameters": {{"action": "search", "text": "Pythonæ•™ç¨‹"}},
  "confidence": 0.90,
  "should_continue": false
}}

6. çŸ¥è¯†åº“æœç´¢ï¼ˆå·²é€‰æ‹©çŸ¥è¯†åº“ï¼‰ï¼š
{{
  "reasoning": "ç”¨æˆ·å·²é€‰æ‹©çŸ¥è¯†åº“ï¼Œä½¿ç”¨ knowledge_search å·¥å…·æ£€ç´¢ç›¸å…³ä¿¡æ¯",
  "tool": "knowledge_search",
  "parameters": {{"query": "Pythonæ•™ç¨‹"}},
  "confidence": 0.95,
  "should_continue": false
}}

ç°åœ¨è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œè¿”å›å®Œæ•´çš„ JSONï¼ˆå¿…é¡»åŒ…å«æ‰€æœ‰5ä¸ªå­—æ®µï¼‰ï¼š
"""

        return prompt

    def _get_tool_descriptions(self, tools: List[str]) -> str:
        """è·å–å·¥å…·æè¿°"""
        descriptions = {
            "calculator": "è®¡ç®—å™¨ - æ‰§è¡Œæ•°å­¦è®¡ç®—,ä¾‹å¦‚: '2+2', '100*5.5'",
            "datetime": "æ—¥æœŸæ—¶é—´ - è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´",
            "knowledge_search": "çŸ¥è¯†åº“æœç´¢ - åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯,éœ€è¦ query å‚æ•°",
            "code_executor": "ä»£ç æ‰§è¡Œ - å®‰å…¨æ‰§è¡Œ Python ä»£ç å¹¶è¿”å›ç»“æœ",
            "browser": "æµè§ˆå™¨ - è®¿é—®ç½‘é¡µã€æœç´¢ä¿¡æ¯ã€æŠ“å–å†…å®¹ã€‚å‚æ•°: action(navigate/scrape/search), url(å¯é€‰), text(å¯é€‰)",
            "file": "æ–‡ä»¶æ“ä½œ - è¯»å–ã€å†™å…¥ã€åˆ—å‡ºæ–‡ä»¶",
        }

        lines = []
        for tool in tools:
            desc = descriptions.get(tool, "æœªçŸ¥å·¥å…·")
            lines.append(f"- {tool}: {desc}")

        return "\n".join(lines) if lines else "æ— å¯ç”¨å·¥å…·"

    async def _call_llm(
        self,
        prompt: str,
        question: str,
        agent_config: Dict[str, Any],
        has_knowledge_base: bool = False  # â† æ–°å¢å‚æ•°
    ) -> str:
        """
        è°ƒç”¨ LLM APIï¼ˆçœŸæ­£çš„ LLM æ¨ç†ï¼‰

        Args:
            prompt: å®Œæ•´çš„æç¤ºè¯ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
            question: åŸå§‹ç”¨æˆ·é—®é¢˜ï¼ˆç”¨äºè§„åˆ™åˆ¤æ–­ï¼‰
            agent_config: Agent é…ç½®
            has_knowledge_base: ç”¨æˆ·æ˜¯å¦é€‰æ‹©äº†çŸ¥è¯†åº“
        """
        try:
            # æ£€æŸ¥æ¨¡å‹é…ç½®
            if not self._model_config:
                logger.error("âŒ LLM æ¨¡å‹æœªé…ç½®ï¼Œæ— æ³•è°ƒç”¨ API")
                raise ValueError("LLM æ¨¡å‹æœªé…ç½®")

            # è·å– API Key å’Œ URL
            api_key = self._model_config.get("api_key")
            api_url = self._model_config.get("api_url")
            model_name = self._model_config.get("name")

            if not api_key or not api_url:
                logger.error(f"âŒ LLM API é…ç½®ä¸å®Œæ•´: api_key={bool(api_key)}, api_url={bool(api_url)}")
                raise ValueError("LLM API é…ç½®ä¸å®Œæ•´")

            logger.info(f"ğŸ¤– [LLM è°ƒç”¨] ä½¿ç”¨æ¨¡å‹: {model_name}")
            logger.info(f"ğŸ“ [LLM è°ƒç”¨] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"ğŸŒ [LLM è°ƒç”¨] API URL: {api_url}")

            # æ„å»ºè¯·æ±‚ä½“ï¼ˆæ™ºè°± AI OpenAI å…¼å®¹æ ¼å¼ï¼‰
            request_data = {
                "model": model_name,
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ Agent çš„å·¥å…·å†³ç­–ä¸“å®¶ã€‚ä¸¥æ ¼éµå®ˆè§„åˆ™ï¼šç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨å·¥å…·æ—¶å¿…é¡»ä½¿ç”¨è¯¥å·¥å…·ã€‚ä½¿ç”¨ Few-shot ç¤ºä¾‹æ¥ç†è§£æ ¼å¼ã€‚"
                    },
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.1,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
                "top_p": 0.9,
                "stream": False
            }

            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }

            # ç›´æ¥è°ƒç”¨ API
            async with httpx.AsyncClient(timeout=30.0) as client:
                logger.info("ğŸŒ [LLM è°ƒç”¨] å‘é€è¯·æ±‚...")
                response = await client.post(
                    api_url,
                    json=request_data,
                    headers=headers
                )

                if response.status_code != 200:
                    logger.error(f"âŒ [LLM è°ƒç”¨] API è¿”å›é”™è¯¯: {response.status_code}")
                    logger.error(f"âŒ [LLM è°ƒç”¨] å“åº”å†…å®¹: {response.text[:500]}")
                    raise ValueError(f"LLM API è¿”å›é”™è¯¯: {response.status_code}")

                # è§£æå“åº”
                response_json = response.json()
                logger.info(f"âœ… [LLM è°ƒç”¨] API è°ƒç”¨æˆåŠŸ")

                # æ™ºè°± AI å“åº”æ ¼å¼: {"choices": [{"message": {"content": "..."}}]}
                if "choices" in response_json and len(response_json["choices"]) > 0:
                    content = response_json["choices"][0]["message"]["content"]
                    logger.info(f"âœ… [LLM å“åº”] å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    logger.info(f"ğŸ“„ [LLM å“åº”] å†…å®¹é¢„è§ˆ: {content[:200]}...")
                    return content
                else:
                    logger.error(f"âŒ [LLM å“åº”] æ ¼å¼é”™è¯¯: {response_json}")
                    raise ValueError("LLM å“åº”æ ¼å¼é”™è¯¯")

        except Exception as e:
            logger.error(f"âŒ LLM API è°ƒç”¨å¤±è´¥: {e}")
            logger.error(f"âŒ [é”™è¯¯è¯¦æƒ…] {type(e).__name__}: {str(e)}")
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise

    async def _call_ias_direct(
        self,
        request: Optional[IASChatRequest],
        question: str,
        has_knowledge_base: bool = False  # â† æ–°å¢å‚æ•°
    ) -> str:
        """
        é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨è§„åˆ™å¼•æ“ï¼ˆå½“ LLM API ä¸å¯ç”¨æ—¶ï¼‰

        Args:
            request: IAS è¯·æ±‚å¯¹è±¡ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
            question: åŸå§‹ç”¨æˆ·é—®é¢˜ï¼ˆç”¨äºè§„åˆ™åˆ¤æ–­ï¼‰
            has_knowledge_base: ç”¨æˆ·æ˜¯å¦é€‰æ‹©äº†çŸ¥è¯†åº“

        æ³¨æ„ï¼šè¿™æ˜¯é™çº§æ–¹æ¡ˆï¼Œä¼˜å…ˆä½¿ç”¨çœŸæ­£çš„ LLM API
        """
        # è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„ IAS API
        # æš‚æ—¶ä½¿ç”¨è§„åˆ™å¼•æ“è¿›è¡Œé™çº§å¤„ç†

        logger.info(f"ğŸ“‹ [è§„åˆ™å¼•æ“] åˆ†æé—®é¢˜: {question[:50]}...")
        logger.info(f"ğŸ“š [è§„åˆ™å¼•æ“] çŸ¥è¯†åº“çŠ¶æ€: {'å·²é€‰æ‹©' if has_knowledge_base else 'æœªé€‰æ‹©'}")

        # âœ… Phase 1 å…³é”®ä¿®å¤: å¦‚æœç”¨æˆ·é€‰æ‹©äº†çŸ¥è¯†åº“ï¼Œä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“æœç´¢
        if has_knowledge_base:
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] æ£€æµ‹åˆ°çŸ¥è¯†åº“é€‰æ‹©ï¼Œä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“æœç´¢")
            # ä½¿ç”¨å®é™…çš„æŸ¥è¯¢å†…å®¹ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
            query_json = json.dumps({"query": question}, ensure_ascii=False)
            return f"""## æ¨ç†
ç”¨æˆ·é€‰æ‹©äº†çŸ¥è¯†åº“ï¼Œåº”è¯¥ä½¿ç”¨çŸ¥è¯†åº“æœç´¢å·¥å…·æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯

## å·¥å…·å†³ç­–
knowledge_search

## å·¥å…·å‚æ•°
{query_json}

## ç½®ä¿¡åº¦
0.95

## ç»§ç»­æ‰§è¡Œ
false
"""

        # ç®€å•è§„åˆ™åˆ¤æ–­ (åŸºäºåŸå§‹é—®é¢˜)
        question_lower = question.lower()

        # âœ… è§„åˆ™ 1.5: ä»£ç æ‰§è¡Œç±»é—®é¢˜ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        # æ£€æµ‹ç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨ä»£ç æ‰§è¡Œ
        if any(keyword in question for keyword in ["ä½¿ç”¨ä»£ç ", "æ‰§è¡Œä»£ç ", "ç”¨ä»£ç ", "ä»£ç æ‰§è¡Œ", "pythonä»£ç ", "è¿è¡Œä»£ç "]):
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] åŒ¹é…è§„åˆ™: ä»£ç æ‰§è¡Œï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰")
            # æå–è®¡ç®—è¡¨è¾¾å¼
            # å°è¯•æå–æ•°å­¦è¡¨è¾¾å¼
            expr_match = re.search(r'(\d+(?:\s*[\*\+\-\/]\s*\d+)+)', question)
            if expr_match:
                expression = expr_match.group(1)
            else:
                expression = question
            code_json = json.dumps({"code": f"print({expression})"}, ensure_ascii=False)
            return f"""## æ¨ç†
ç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨ä»£ç æ‰§è¡Œè®¡ç®—,å¿…é¡»ä½¿ç”¨ code_executor å·¥å…·

## å·¥å…·å†³ç­–
code_executor

## å·¥å…·å‚æ•°
{code_json}

## ç½®ä¿¡åº¦
0.98

## ç»§ç»­æ‰§è¡Œ
false
"""

        # è§„åˆ™ 1: è®¡ç®—ç±»é—®é¢˜
        if any(keyword in question for keyword in ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/", "="]):
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] åŒ¹é…è§„åˆ™: è®¡ç®—")
            expression_json = json.dumps({"expression": question}, ensure_ascii=False)
            return f"""## æ¨ç†
ç”¨æˆ·é—®é¢˜æ¶‰åŠæ•°å­¦è®¡ç®—,éœ€è¦ä½¿ç”¨è®¡ç®—å™¨å·¥å…·

## å·¥å…·å†³ç­–
calculator

## å·¥å…·å‚æ•°
{expression_json}

## ç½®ä¿¡åº¦
0.95

## ç»§ç»­æ‰§è¡Œ
false
"""

        # è§„åˆ™ 2: æ—¶é—´æ—¥æœŸç±»é—®é¢˜
        elif any(keyword in question for keyword in ["æ—¶é—´", "æ—¥æœŸ", "å‡ ç‚¹", "ç°åœ¨", "ä»Šå¤©", "å‡ ç‚¹äº†"]):
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] åŒ¹é…è§„åˆ™: æ—¶é—´æ—¥æœŸ")
            return """## æ¨ç†
ç”¨æˆ·è¯¢é—®å½“å‰æ—¶é—´æˆ–æ—¥æœŸ,éœ€è¦ä½¿ç”¨æ—¥æœŸæ—¶é—´å·¥å…·

## å·¥å…·å†³ç­–
datetime

## å·¥å…·å‚æ•°
{}

## ç½®ä¿¡åº¦
0.95

## ç»§ç»­æ‰§è¡Œ
false
"""

        # è§„åˆ™ 3: æœç´¢ç±»é—®é¢˜
        elif any(keyword in question_lower for keyword in ["æœç´¢", "search", "æŸ¥æ‰¾", "çŸ¥è¯†", "ä¿¡æ¯", "ä»€ä¹ˆæ˜¯"]):
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] åŒ¹é…è§„åˆ™: æœç´¢")
            query_json = json.dumps({"query": question}, ensure_ascii=False)
            return f"""## æ¨ç†
ç”¨æˆ·éœ€è¦æŸ¥æ‰¾ä¿¡æ¯,åº”è¯¥ä½¿ç”¨çŸ¥è¯†åº“æœç´¢å·¥å…·

## å·¥å…·å†³ç­–
knowledge_search

## å·¥å…·å‚æ•°
{query_json}

## ç½®ä¿¡åº¦
0.90

## ç»§ç»­æ‰§è¡Œ
false
"""

        # è§„åˆ™ 4: é—®å€™ç±»é—®é¢˜ï¼ˆç›´æ¥å›ç­”ï¼‰
        elif any(keyword in question for keyword in ["ä½ å¥½", "hello", "hi", "å—¨", "æ‚¨å¥½"]):
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] åŒ¹é…è§„åˆ™: é—®å€™")
            return """## æ¨ç†
ç”¨æˆ·åœ¨æ‰“æ‹›å‘¼,åº”è¯¥å‹å¥½å›åº”,ä¸éœ€è¦ä½¿ç”¨å·¥å…·

## å·¥å…·å†³ç­–
none

## å·¥å…·å‚æ•°
{}

## ç½®ä¿¡åº¦
0.95

## ç»§ç»­æ‰§è¡Œ
false
"""

        else:
            # é»˜è®¤: ä¸ä½¿ç”¨å·¥å…·,å°è¯•ç›´æ¥å›ç­”
            logger.info("ğŸ¯ [è§„åˆ™å¼•æ“] ä½¿ç”¨é»˜è®¤è§„åˆ™")
            return """## æ¨ç†
é—®é¢˜å¯ä»¥ç›´æ¥å›ç­”æˆ–ä¸éœ€è¦ç‰¹å®šå·¥å…·,å°†å°è¯•æä¾›æœ‰å¸®åŠ©çš„å›å¤

## å·¥å…·å†³ç­–
none

## å·¥å…·å‚æ•°
{}

## ç½®ä¿¡åº¦
0.70

## ç»§ç»­æ‰§è¡Œ
false
"""


    def _parse_decision(self, response: str, available_tools: List[str]) -> Dict[str, Any]:
        """è§£æ LLM å“åº”ä¸ºå†³ç­–ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""
        try:
            response_stripped = response.strip()

            # ğŸ” æ ¼å¼ 1: æ ‡å‡† JSONï¼ˆä»¥ { å¼€å¤´ï¼‰
            if response_stripped.startswith('{'):
                logger.info(f"ğŸ“‹ [æ ¼å¼æ£€æµ‹] æ ‡å‡† JSON")
                return self._parse_json_format(response_stripped, available_tools)

            # ğŸ” æ ¼å¼ 2: JSON ä»£ç å—ï¼ˆ```json ... ```ï¼‰
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                logger.info(f"ğŸ“‹ [æ ¼å¼æ£€æµ‹] JSON ä»£ç å—")
                return self._parse_json_format(json_match.group(1), available_tools)

            # ğŸ” æ ¼å¼ 3: å·¥å…·å + JSON å‚æ•°ï¼ˆæ–°æ ¼å¼ï¼‰
            # ä¾‹å¦‚: browser\n{"action": "search", "url": "..."}
            lines = response_stripped.split('\n', 1)
            if len(lines) == 2:
                first_line = lines[0].strip()
                second_line = lines[1].strip()

                if first_line in available_tools and second_line.startswith('{'):
                    logger.info(f"ğŸ“‹ [æ ¼å¼æ£€æµ‹] å·¥å…·å+å‚æ•°æ ¼å¼")
                    logger.info(f"   å·¥å…·: {first_line}")

                    try:
                        parameters = json.loads(second_line)
                        logger.info(f"   å‚æ•°: {parameters}")

                        return {
                            "reasoning": f"ä½¿ç”¨ {first_line} å·¥å…·æ‰§è¡Œä»»åŠ¡",
                            "tool": first_line,
                            "parameters": parameters,
                            "confidence": 0.9,
                            "should_continue": False
                        }
                    except json.JSONDecodeError as e:
                        logger.warning(f"âš ï¸ å‚æ•°è§£æå¤±è´¥: {e}")

            # ğŸ” æ ¼å¼ 4: Markdown æ ¼å¼ï¼ˆåŒ…å« ## æ ‡é¢˜ï¼‰
            has_markdown_sections = any(
                f"## {section}" in response
                for section in ["æ¨ç†", "å·¥å…·å†³ç­–", "å·¥å…·å‚æ•°", "ç½®ä¿¡åº¦", "ç»§ç»­æ‰§è¡Œ"]
            )

            if has_markdown_sections:
                logger.info(f"ğŸ“‹ [æ ¼å¼æ£€æµ‹] Markdown æ ¼å¼")
                return self._parse_markdown_format(response, available_tools)

            # ğŸ” æ ¼å¼ 5: ç›´æ¥ç­”æ¡ˆï¼ˆå…œåº•ï¼‰
            logger.info(f"ğŸ“‹ [æ ¼å¼æ£€æµ‹] ç›´æ¥ç­”æ¡ˆï¼ˆéç»“æ„åŒ–æ–‡æœ¬ï¼‰")
            logger.info(f"ğŸ“„ [ç›´æ¥ç­”æ¡ˆ] å†…å®¹: {response[:100]}...")

            return {
                "reasoning": response,
                "tool": None,
                "parameters": {},
                "confidence": 0.9,
                "should_continue": False
            }

        except Exception as e:
            logger.error(f"âŒ è§£æå†³ç­–å¤±è´¥: {e}")
            logger.error(f"ğŸ“„ å“åº”å†…å®¹: {response[:500]}")
            return {
                "reasoning": response[:200] if response else "è§£æå¤±è´¥",
                "tool": None,
                "parameters": {},
                "confidence": 0.5,
                "should_continue": False
            }

    def _parse_json_format(self, json_str: str, available_tools: List[str]) -> Dict[str, Any]:
        """è§£ææ ‡å‡† JSON æ ¼å¼"""
        try:
            data = json.loads(json_str)
            if data is None:
                data = {}

            logger.info(f"âœ… [JSON è§£æ] æˆåŠŸ")

            tool = data.get("tool")
            if tool is None or tool == "null":
                tool = None
            elif isinstance(tool, str):
                tool = tool.strip().lower()
                if tool in ["none", "null"]:
                    tool = None

            if tool and tool not in available_tools:
                logger.warning(f"âš ï¸ å·¥å…· {tool} ä¸å¯ç”¨")
                tool = None

            return {
                "reasoning": data.get("reasoning", ""),
                "tool": tool,
                "parameters": data.get("parameters", {}),
                "confidence": float(data.get("confidence", 0.8)),
                "should_continue": bool(data.get("should_continue", False))
            }

        except json.JSONDecodeError as e:
            logger.error(f"âŒ [JSON è§£æ] å¤±è´¥: {e}")
            raise

    def _parse_markdown_format(self, response: str, available_tools: List[str]) -> Dict[str, Any]:
        """è§£æ Markdown æ ¼å¼"""
        reasoning = self._extract_section(response, "æ¨ç†")
        tool = self._extract_section(response, "å·¥å…·å†³ç­–").strip().lower()
        parameters_str = self._extract_section(response, "å·¥å…·å‚æ•°")
        confidence_str = self._extract_section(response, "ç½®ä¿¡åº¦").strip()
        should_continue_str = self._extract_section(response, "ç»§ç»­æ‰§è¡Œ").strip().lower()

        # è§£æå‚æ•°
        try:
            parameters = json.loads(parameters_str) if parameters_str and parameters_str != "{}" else {}
        except json.JSONDecodeError:
            logger.warning(f"âš ï¸ å‚æ•°è§£æå¤±è´¥")
            parameters = {}

        # è§£æç½®ä¿¡åº¦
        try:
            confidence = float(confidence_str) if confidence_str else 0.5
        except ValueError:
            confidence = 0.5

        should_continue = should_continue_str in ["true", "yes", "æ˜¯"]

        # éªŒè¯å·¥å…·
        if tool and tool != "none" and tool not in available_tools:
            logger.warning(f"âš ï¸ å·¥å…· {tool} ä¸å¯ç”¨")
            tool = "none"

        return {
            "reasoning": reasoning,
            "tool": tool if tool != "none" else None,
            "parameters": parameters,
            "confidence": confidence,
            "should_continue": should_continue
        }

    def _extract_section(self, text: str, section_name: str) -> str:
        """æå–æ–‡æœ¬ä¸­çš„æŸä¸ªéƒ¨åˆ†"""
        # æŸ¥æ‰¾ ## section_name
        marker = f"## {section_name}"
        if marker not in text:
            return ""

        start = text.index(marker) + len(marker)
        # æŸ¥æ‰¾ä¸‹ä¸€ä¸ª ## æˆ–æ–‡æœ¬ç»“æŸ
        next_marker = text.find("##", start)
        if next_marker == -1:
            return text[start:].strip()
        else:
            return text[start:next_marker].strip()

    def _fallback_decision(
        self,
        question: str,
        available_tools: List[str],
        previous_steps: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        é™çº§å†³ç­– (å½“ LLM è°ƒç”¨å¤±è´¥æ—¶)

        ä½¿ç”¨ç®€å•è§„åˆ™è¿›è¡Œå·¥å…·é€‰æ‹©
        """
        question_lower = question.lower()

        # âœ… è§„åˆ™ 0: ä»£ç æ‰§è¡Œï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if any(word in question for word in ["ä½¿ç”¨ä»£ç ", "æ‰§è¡Œä»£ç ", "ç”¨ä»£ç ", "ä»£ç æ‰§è¡Œ"]) and "code_executor" in available_tools:
            # æå–è¡¨è¾¾å¼
            expr_match = re.search(r'(\d+(?:\s*[\*\+\-\/]\s*\d+)+)', question)
            if expr_match:
                expression = expr_match.group(1)
                code = f"print({expression})"
            else:
                code = question
            return {
                "reasoning": "ç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨ä»£ç æ‰§è¡Œ",
                "tool": "code_executor",
                "parameters": {"code": code},
                "confidence": 0.95,
                "should_continue": False
            }

        # è§„åˆ™ 1: è®¡ç®—
        if any(word in question for word in ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/"]) and "calculator" in available_tools:
            return {
                "reasoning": "é—®é¢˜æ¶‰åŠæ•°å­¦è®¡ç®—,ä½¿ç”¨è®¡ç®—å™¨",
                "tool": "calculator",
                "parameters": {"expression": question},
                "confidence": 0.70,
                "should_continue": False
            }

        # è§„åˆ™ 2: æ—¥æœŸæ—¶é—´
        if any(word in question for word in ["æ—¶é—´", "æ—¥æœŸ", "å‡ ç‚¹", "ä»Šå¤©", "ç°åœ¨"]) and "datetime" in available_tools:
            return {
                "reasoning": "é—®é¢˜è¯¢é—®å½“å‰æ—¶é—´,ä½¿ç”¨æ—¥æœŸæ—¶é—´å·¥å…·",
                "tool": "datetime",
                "parameters": {},
                "confidence": 0.80,
                "should_continue": False
            }

        # è§„åˆ™ 3: çŸ¥è¯†åº“æœç´¢
        if any(word in question for word in ["æœç´¢", "æŸ¥æ‰¾", "çŸ¥è¯†", "ä¿¡æ¯"]) and "knowledge_search" in available_tools:
            return {
                "reasoning": "é—®é¢˜éœ€è¦æœç´¢ä¿¡æ¯,ä½¿ç”¨çŸ¥è¯†åº“",
                "tool": "knowledge_search",
                "parameters": {"query": question},
                "confidence": 0.70,
                "should_continue": False
            }

        # é»˜è®¤: ä¸ä½¿ç”¨å·¥å…·
        return {
            "reasoning": "é—®é¢˜å¯ä»¥ç›´æ¥å›ç­”,ä¸éœ€è¦å·¥å…·",
            "tool": None,
            "parameters": {},
            "confidence": 0.60,
            "should_continue": False
        }

    async def generate_final_answer(
        self,
        question: str,
        tools_called: List[Dict[str, Any]],
        agent_config: Dict[str, Any]
    ) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

        Args:
            question: ç”¨æˆ·é—®é¢˜
            tools_called: å·¥å…·è°ƒç”¨è®°å½•
            agent_config: Agent é…ç½®

        Returns:
            æœ€ç»ˆç­”æ¡ˆ
        """
        # è¿™ä¸ªæ–¹æ³•åœ¨ agent_service.py ä¸­å·²å®ç°
        # è¿™é‡Œåªæ˜¯æ¥å£å®šä¹‰
        raise NotImplementedError("ä½¿ç”¨ AgentService._generate_final_answer ä»£æ›¿")
